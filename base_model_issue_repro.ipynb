{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "538729be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "def kwargs_ser_safe(kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        if isinstance(value, NewBaseModel):\n",
    "            kwargs[key] = {'BaseModel': value.state_dict()[\"_metadata\"]}\n",
    "        elif isinstance(value, dict):\n",
    "            kwargs[key] = kwargs_ser_safe(value)\n",
    "        \n",
    "        if isinstance(value, nn.Module):\n",
    "            print(f\"Dangerous value: {value}\")\n",
    "            warnings.warn(\"Warning: attempting to initialize a BaseModel with non-serializable kwargs. \"\n",
    "                          \"In general, we recommend removing all nn.Modules from your model's init args.\"\n",
    "                          )\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "#  recursively loop through all torch.loaded metadata to turn deserialized init kwargs back into nested BaseModels\n",
    "def metadata_to_base_models(kwargs):\n",
    "    #pprint(kwargs)\n",
    "    for key, value in kwargs.items():\n",
    "        if isinstance(value, dict):\n",
    "            base_model_init_kwargs = value.pop('BaseModel', None)\n",
    "            if base_model_init_kwargs is not None:\n",
    "                if 'args' in base_model_init_kwargs:\n",
    "                    init_args = base_model_init_kwargs.pop('args')\n",
    "                else:\n",
    "                    init_args = []\n",
    "                base_model_cls = BaseModel._models[base_model_init_kwargs['_name'].lower()]\n",
    "                kwargs[key] = base_model_cls(*init_args, **base_model_init_kwargs)\n",
    "            else:\n",
    "                #import pdb; pdb.set_trace()\n",
    "                kwargs[key] = metadata_to_base_models(value)\n",
    "                \n",
    "    return kwargs\n",
    "\n",
    "class NewBaseModel(torch.nn.Module):\n",
    "    \"\"\"Base class for all Models\n",
    "\n",
    "    This class has two main purposes:\n",
    "    * It monitors the creation of subclasses that are automatically registered \n",
    "      for users to use by name using the library's config system\n",
    "    * When a new instance of this class is created, the init call is intercepted\n",
    "      so we can store the parameters used to create the instance.\n",
    "      This makes it possible to save trained models along with their init parameters,\n",
    "      and therefore load saved modes easily.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Any BaseModel instance can be versioned using the _version class attribute. \n",
    "    This can be used as a sanity check when loading models from checkpoints to verify the \n",
    "    model hasn't been updated since.\n",
    "    \"\"\"\n",
    "    _models = dict()\n",
    "    _version = '0.1.0'\n",
    "\n",
    "    def __init_subclass__(cls, name=None, **kwargs):\n",
    "        \"\"\"When a subclass is created, register it in _models\n",
    "        We look for an existing name attribute. \n",
    "        If not give, then we use the class' name.\n",
    "        \"\"\"\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        if name is not None:\n",
    "            BaseModel._models[name.lower()] = cls\n",
    "            cls._name = name\n",
    "        else:\n",
    "            # warnings.warn(f'Creating a subclass of BaseModel {cls.__name__} with no name, initializing with {cls.__name__}.')\n",
    "            BaseModel._models[cls.__name__.lower()] = cls\n",
    "            cls._name = cls.__name__\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        \"\"\"Verify arguments and save init kwargs for loading/saving\n",
    "\n",
    "        We inspect the class' signature and check for unused parameters, or \n",
    "        parameters not passed. \n",
    "\n",
    "        We store all the args and kwargs given so we can duplicate the instance transparently.\n",
    "        \"\"\"\n",
    "        sig = inspect.signature(cls)\n",
    "        metadata = cls._validate_and_store_args(sig, args, kwargs)\n",
    "        instance = super().__new__(cls)\n",
    "        instance._metadata = metadata\n",
    "\n",
    "        return instance\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_and_store_args(cls, sig, args, kwargs):\n",
    "        class_name = cls.__name__\n",
    "\n",
    "        # ensure that if metadata contains another BaseModel object, we convert that in a way that is loadable with ``weights_only=False``\n",
    "        '''for i, arg in enumerate(args):\n",
    "            if isinstance(arg, BaseModel):\n",
    "                args[i] = {'BaseModel': arg._metadata}\n",
    "        print(args)'''\n",
    "        kwargs = kwargs_ser_safe(kwargs)\n",
    "\n",
    "        metadata = {\"_args\": args, \"_kwargs\": kwargs, \"_version\": cls._version, \"_name\": class_name}\n",
    "\n",
    "        verbose = kwargs.get('verbose', False)\n",
    "        # Unexpected arguments: verify that given parameters are actually arguments of the model\n",
    "        for key in kwargs:\n",
    "            if key not in sig.parameters and verbose:\n",
    "                warnings.warn(f\"Given argument '{key}' that isn't in the signature of class {class_name}.\")\n",
    "\n",
    "        # Fill in default arguments: check for model arguments not specified in the configuration\n",
    "        for key, param in sig.parameters.items():\n",
    "            if param.default is not inspect._empty and key not in kwargs:\n",
    "                kwargs[key] = param.default\n",
    "                if verbose:\n",
    "                    print(f\"Keyword argument {key} not specified for class {class_name},  using default={param.default}.\")\n",
    "        return metadata\n",
    "\n",
    "    def state_dict(self, destination: dict=None, prefix: str='', keep_vars: bool=False):\n",
    "        \"\"\"\n",
    "        state_dict subclasses nn.Module.state_dict() and adds a metadata field\n",
    "        to track the model version and ensure only compatible saves are loaded.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        destination : dict, optional\n",
    "            If provided, the state of module will\n",
    "            be updated into the dict and the same object is returned.\n",
    "            Otherwise, an OrderedDict will be created and returned, by default None\n",
    "        prefix : str, optional\n",
    "            a prefix added to parameter and buffer\n",
    "            names to compose the keys in state_dict, by default ``''``\n",
    "        keep_vars (bool, optional): by default the torch.Tensors\n",
    "            returned in the state dict are detached from autograd. \n",
    "            If True, detaching will not be performed, by default False\n",
    "\n",
    "        \"\"\"\n",
    "        state_dict = super().state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)\n",
    "        if state_dict.get('_metadata') == None:\n",
    "            state_dict['_metadata'] = self._metadata\n",
    "            print(f\"storing metadata: for {self._name}\")\n",
    "            pprint(state_dict['_metadata'])\n",
    "\n",
    "        else:\n",
    "            print(f\"original metadata for {self._name}\")\n",
    "            pprint(state_dict['_metadata'])\n",
    "            warnings.warn(\"Attempting to update metadata for a module with metadata already in self.state_dict()\")\n",
    "            state_dict['_metadata'].update(self._metadata)\n",
    "            print(f\"storing metadata for {self._name}\")\n",
    "            pprint(state_dict['_metadata'])\n",
    "        return state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict, strict=True, assign=False):\n",
    "        \"\"\"load_state_dict subclasses nn.Module.load_state_dict() and adds a metadata field\n",
    "        to track the model version and ensure only compatible saves are loaded.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_dict : dict\n",
    "            state dictionary generated by ``nn.Module.state_dict()``\n",
    "        strict : bool, optional\n",
    "            whether to strictly enforce that the keys in ``state_dict``\n",
    "            match the keys returned by this module's, by default True.\n",
    "        assign : bool, optional\n",
    "            whether to assign items in the state dict to their corresponding keys\n",
    "            in the module instead of copying them inplace into the module's current\n",
    "            parameters and buffers. When False, the properties of the tensors in the\n",
    "            current module are preserved while when True, the properties of the Tensors\n",
    "            in the state dict are preserved, by default False\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _type_\n",
    "            _description_\n",
    "        \"\"\"\n",
    "        metadata = state_dict.pop('_metadata', None)\n",
    "\n",
    "        if metadata is not None:\n",
    "            saved_version = metadata.get('_version', None)\n",
    "            if saved_version is None:\n",
    "                warnings.warn(f\"Saved instance of {self.__class__} has no stored version attribute.\")\n",
    "            if saved_version != self._version:\n",
    "                warnings.warn(f\"Attempting to load a {self.__class__} of version {saved_version},\"\n",
    "                              f\"But current version of {self.__class__} is {saved_version}\")\n",
    "            # remove state dict metadata at the end to ensure proper loading with PyTorch module\n",
    "        super().load_state_dict(state_dict, strict=strict, assign=assign)\n",
    "        self._metadata = metadata\n",
    "    \n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, checkpoint_path, map_location=None, strict=True, assign=False):\n",
    "        # Load the checkpoint safely: change weights_only to False if you want to load the full checkpoint\n",
    "        state_dict = torch.load(checkpoint_path, map_location=map_location, weights_only=True)\n",
    "\n",
    "        metadata = state_dict.get('_metadata', dict())\n",
    "        version = metadata.get('_version', None)\n",
    "\n",
    "        if version is not None and hasattr(cls, '_version') and version != cls._version:\n",
    "            warnings.warn(f'Checkpoint saved for version {version} of class {cls.__name__} but current code is version {cls._version}')\n",
    "\n",
    "        metadata = metadata_to_base_models(metadata)\n",
    "\n",
    "        init_args = metadata.get('_args', list())\n",
    "        init_kwargs = metadata.get('_kwargs', dict())\n",
    "        instance = cls(*init_args, **init_kwargs)\n",
    "\n",
    "        instance.load_state_dict(state_dict, strict=strict, assign=assign)\n",
    "        instance._metadata = metadata\n",
    "\n",
    "        return instance\n",
    "\n",
    "\n",
    "def available_models():\n",
    "    \"\"\"List the available neural operators\"\"\"\n",
    "    return list(BaseModel._models.keys())\n",
    "\n",
    "\n",
    "def get_model(config):\n",
    "    \"\"\"Returns an instantiated model for the given config\n",
    "\n",
    "    * Reads the model to be used from config['arch']\n",
    "    * Adjusts config[\"arch\"][\"data_channels\"] accordingly if multi-grid patching is used\n",
    "\n",
    "    Also prints warnings for safety, in case::\n",
    "    * some given arguments aren't actually used by the model\n",
    "    * some keyword arguments of the models aren't provided by the config\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : Bunch or dict-like\n",
    "        configuration, must have\n",
    "        arch = config['arch'] (string)\n",
    "        and the corresponding config[arch] (a subdict with the kwargs of the model)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : nn.Module\n",
    "        the instanciated module\n",
    "    \"\"\"\n",
    "    arch = config[\"arch\"].lower()\n",
    "    config_arch = config.get(arch)\n",
    "\n",
    "    # Set the number of input channels depending on channels in data + mg patching\n",
    "    data_channels = config_arch.pop(\"data_channels\")\n",
    "    try:\n",
    "        patching_levels = config[\"patching\"][\"levels\"]\n",
    "    except KeyError:\n",
    "        patching_levels = 0\n",
    "    if patching_levels:\n",
    "        data_channels *= patching_levels + 1\n",
    "    config_arch[\"in_channels\"] = data_channels\n",
    "\n",
    "    # Dispatch model creation\n",
    "    try:\n",
    "        return BaseModel._models[arch](**config_arch)\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Got config.arch={arch}, expected one of {available_models()}.\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5344de36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dangerous value: DummyModel(\n",
      "  (net): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "storing metadata: for DummyModel\n",
      "{'_args': (10,), '_kwargs': {}, '_name': 'DummyModel', '_version': '0.1.0'}\n",
      "original metadata for NestedBaseModel\n",
      "{'_args': (10,), '_kwargs': {}, '_name': 'DummyModel', '_version': '0.1.0'}\n",
      "storing metadata for NestedBaseModel\n",
      "{'_args': (),\n",
      " '_kwargs': {'submodel': DummyModel(\n",
      "  (net): Linear(in_features=10, out_features=1, bias=True)\n",
      ")},\n",
      " '_name': 'NestedBaseModel',\n",
      " '_version': '0.1.0'}\n",
      "loaded_state_dict=OrderedDict([('submodel.net.weight', tensor([[ 0.0458, -0.1569, -0.2734, -0.0687,  0.0055, -0.0105, -0.2497, -0.0324,\n",
      "          0.1063,  0.0282]])), ('submodel.net.bias', tensor([-0.2256])), ('_metadata', {'_args': (), '_kwargs': {'submodel': DummyModel(\n",
      "  (net): Linear(in_features=10, out_features=1, bias=True)\n",
      ")}, '_version': '0.1.0', '_name': 'NestedBaseModel'})])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88490/3134298206.py:19: UserWarning: Warning: attempting to initialize a BaseModel with non-serializable kwargs. In general, we recommend removing all nn.Modules from your model's init args.\n",
      "  warnings.warn(\"Warning: attempting to initialize a BaseModel with non-serializable kwargs. \"\n",
      "/tmp/ipykernel_88490/3134298206.py:148: UserWarning: Attempting to update metadata for a module with metadata already in self.state_dict()\n",
      "  warnings.warn(\"Attempting to update metadata for a module with metadata already in self.state_dict()\")\n"
     ]
    }
   ],
   "source": [
    "from neuralop.tests.test_utils import DummyModel\n",
    "\n",
    "class NestedBaseModel(NewBaseModel, name=\"NestedBaseModel\"):\n",
    "    def __init__(self, submodel):\n",
    "        super().__init__()\n",
    "        self.submodel = submodel\n",
    "    \n",
    "nested_model = NestedBaseModel(submodel=DummyModel(10))\n",
    "torch.save(nested_model.state_dict(), \"./nested_state_dict.pt\")\n",
    "loaded_state_dict = torch.load(\"./nested_state_dict.pt\", weights_only=False)\n",
    "#nested_model.\n",
    "print(f\"{loaded_state_dict=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb917f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storing metadata: for DummyModel\n",
      "{'_args': (10,), '_kwargs': {}, '_name': 'DummyModel', '_version': '0.1.0'}\n",
      "Dangerous value: DummyModel(\n",
      "  (net): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "storing metadata: for DummyModel\n",
      "{'_args': (10,), '_kwargs': {}, '_name': 'DummyModel', '_version': '0.1.0'}\n",
      "original metadata for NestedBaseModel\n",
      "{'_args': (10,), '_kwargs': {}, '_name': 'DummyModel', '_version': '0.1.0'}\n",
      "storing metadata for NestedBaseModel\n",
      "{'_args': (),\n",
      " '_kwargs': {'submodel': {'BaseModel': <Recursion on dict with id=124553019166272>}},\n",
      " '_name': 'NestedBaseModel',\n",
      " '_version': '0.1.0'}\n",
      "OrderedDict([('submodel.net.weight',\n",
      "              tensor([[-0.1954,  0.3121, -0.1266, -0.2431,  0.2976, -0.2593, -0.1369,  0.0763,\n",
      "          0.2793, -0.3128]])),\n",
      "             ('submodel.net.bias', tensor([0.1918])),\n",
      "             ('_metadata',\n",
      "              {'_args': (),\n",
      "               '_kwargs': {'submodel': {'BaseModel': <Recursion on dict with id=124558929442496>}},\n",
      "               '_name': 'NestedBaseModel',\n",
      "               '_version': '0.1.0'})])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/myneurop/neuralop/models/base_model.py:19: UserWarning: Warning: attempting to initialize a BaseModel with non-serializable kwargs. In general, we recommend removing all nn.Modules from your model's init args.\n",
      "  warnings.warn(\"Warning: attempting to initialize a BaseModel with non-serializable kwargs. \"\n",
      "/home/dave/myneurop/neuralop/models/base_model.py:148: UserWarning: Attempting to update metadata for a module with metadata already in self.state_dict()\n",
      "  warnings.warn(\"Attempting to update metadata for a module with metadata already in self.state_dict()\")\n"
     ]
    }
   ],
   "source": [
    "from neuralop.tests.test_utils import DummyModel\n",
    "from neuralop.models.base_model import BaseModel\n",
    "\n",
    "class NestedBaseModel(BaseModel, name=\"NestedBaseModel\"):\n",
    "    def __init__(self, submodel):\n",
    "        super().__init__()\n",
    "        self.submodel = submodel\n",
    "    \n",
    "nested_model = NestedBaseModel(submodel=DummyModel(10))\n",
    "torch.save(nested_model.state_dict(), \"./nested_state_dict.pt\")\n",
    "loaded_state_dict = torch.load(\"./nested_state_dict.pt\", weights_only=True)\n",
    "#nested_model.\n",
    "pprint(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40338068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('submodel.net.weight',\n",
       "              tensor([[ 0.1250,  0.1695,  0.0170,  0.1084,  0.0826, -0.2519,  0.1799, -0.1548,\n",
       "                        0.1501, -0.1448]])),\n",
       "             ('submodel.net.bias', tensor([-0.0070])),\n",
       "             ('_metadata',\n",
       "              {'_args': (),\n",
       "               '_kwargs': {'submodel': {'BaseModel': {...}}},\n",
       "               '_version': '0.1.0',\n",
       "               '_name': 'NestedBaseModel'})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac54e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1ec14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
